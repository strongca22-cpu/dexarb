# DEX Arbitrage Bot - Phase 1 & 2 Optimization Plan
## Static Whitelist, Pool Quality Scoring & Multicall Batching

**Version**: 1.1
**Date**: 2026-01-28
**Updated**: 2026-01-29
**Timeline**: 2 weeks (Phase 1: Week 1, Phase 2: Week 2)
**Goal**: Reduce phantom spreads by 80% and RPC calls by 95%

### Implementation Status

| Phase | Task | Status | Date |
|-------|------|--------|------|
| 1.1 | Static Whitelist/Blacklist | **COMPLETED** | 2026-01-29 |
| 1.2 | Enhanced Liquidity Thresholds | Not started | â€” |
| 1.3 | Pool Quality Scoring | Not started | â€” |
| 2.1 | Multicall3 Integration | Not started | â€” |
| 2.2 | Adaptive Batch Sizing | Not started | â€” |

See `docs/session_summaries/2026-01-29_phase1_1_whitelist.md` for Phase 1.1 details.

---

## ðŸ“Š EXECUTIVE SUMMARY

### **Current Situation**

```
PROBLEMS:
âŒ 80% false positive rate (344 phantom out of 430 opportunities/hour)
âŒ 860 RPC calls/hour (2 Quoter calls Ã— 430 opportunities)
âŒ 200ms latency per opportunity check
âŒ No learning from failed executions
âŒ No formal pool quality tracking

ARCHITECTURE:
â”œâ”€ Pools discovered dynamically from factory
â”œâ”€ Basic pre-filtering (fee tier, liquidity)
â”œâ”€ Sequential Quoter verification (expensive)
â”œâ”€ No whitelist or quality scoring
â””â”€ JSON-based shared state
```

### **Solution Overview**

```
PHASE 1: INTELLIGENT FILTERING (Week 1)
â”œâ”€ Static whitelist/blacklist (JSON config)
â”œâ”€ Enhanced liquidity thresholds (per-tier)
â”œâ”€ Pool quality scoring (learns from history)
â””â”€ Expected: 62% reduction in false positives

PHASE 2: MULTICALL BATCHING (Week 2)
â”œâ”€ Multicall3 integration (batch RPC calls)
â”œâ”€ Adaptive batch sizing
â”œâ”€ 95% RPC call reduction
â””â”€ 95% latency reduction

COMBINED IMPACT:
â”œâ”€ False positives: 80% â†’ 18% (-77%)
â”œâ”€ RPC calls: 860/hour â†’ 10-20/hour (-97%)
â”œâ”€ Latency: 200ms â†’ 10ms (-95%)
â””â”€ Quality: Self-improving over time
```

---

## ðŸ—ï¸ ARCHITECTURE OVERVIEW

### **Current Architecture**

```
CURRENT FLOW:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TRADING_PAIRS env var (7 pairs)                      â”‚
â”‚    â†“                                                     â”‚
â”‚ 2. Factory lookup (getPool for each fee tier)           â”‚
â”‚    â†“                                                     â”‚
â”‚ 3. PRE-FILTER in detector.rs:                           â”‚
â”‚    â€¢ Skip fee >= 10000 (1% tier)                        â”‚
â”‚    â€¢ Skip liquidity < 1000 (too low!)                   â”‚
â”‚    â€¢ Skip invalid prices                                â”‚
â”‚    â†“                                                     â”‚
â”‚ 4. DETECTION:                                           â”‚
â”‚    â€¢ Calculate spread from tick data                    â”‚
â”‚    â€¢ Check net_profit > min_profit_usd                  â”‚
â”‚    â†“                                                     â”‚
â”‚ 5. POST-FILTER in executor.rs:                          â”‚
â”‚    â€¢ Quoter check leg 1 (RPC call)                      â”‚
â”‚    â€¢ Execute leg 1 if profitable                        â”‚
â”‚    â€¢ Quoter check leg 2 (RPC call)                      â”‚
â”‚    â€¢ Execute leg 2                                      â”‚
â”‚    â†“                                                     â”‚
â”‚ 6. LOG to TradeResult (JSON)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DATA AVAILABLE:
â”œâ”€ Pool addresses (from factory)
â”œâ”€ Liquidity values (from slot0)
â”œâ”€ Tick data (sqrtPriceX96)
â”œâ”€ Fee tiers (500, 3000, 10000)
â””â”€ Execution history (TradeResult JSON)
```

### **Enhanced Architecture (Phase 1 & 2)**

```
ENHANCED FLOW:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TRADING_PAIRS env var (unchanged)                    â”‚
â”‚    â†“                                                     â”‚
â”‚ 2. Factory lookup (unchanged)                           â”‚
â”‚    â†“                                                     â”‚
â”‚ 3. âœ¨ WHITELIST VALIDATION (NEW - Phase 1.1)            â”‚
â”‚    â€¢ Load pools_whitelist.json                          â”‚
â”‚    â€¢ Check pool.address in whitelist                    â”‚
â”‚    â€¢ Check pool.tier not blacklisted                    â”‚
â”‚    â€¢ Skip if not whitelisted or blacklisted             â”‚
â”‚    â†“ IMPACT: -40% phantoms                              â”‚
â”‚                                                          â”‚
â”‚ 4. PRE-FILTER (enhanced - Phase 1.2)                    â”‚
â”‚    â€¢ Skip fee >= 10000 (existing)                       â”‚
â”‚    â€¢ Skip liquidity < ENHANCED_THRESHOLD                â”‚
â”‚      - 0.05% tier: >5B liquidity                        â”‚
â”‚      - 0.30% tier: >3B liquidity                        â”‚
â”‚    â€¢ Skip invalid prices (existing)                     â”‚
â”‚    â†“ IMPACT: -20% more phantoms                         â”‚
â”‚                                                          â”‚
â”‚ 5. âœ¨ POOL QUALITY CHECK (NEW - Phase 1.3)              â”‚
â”‚    â€¢ Load pool_scores.json                              â”‚
â”‚    â€¢ Get score for pool.address                         â”‚
â”‚    â€¢ Skip if score < 0.4                                â”‚
â”‚    â€¢ Prioritize high-scoring pools                      â”‚
â”‚    â†“ IMPACT: -15% more phantoms, better prioritization  â”‚
â”‚                                                          â”‚
â”‚ 6. DETECTION (unchanged)                                â”‚
â”‚    â€¢ Calculate spread                                   â”‚
â”‚    â€¢ Check profit threshold                             â”‚
â”‚    â†“ Result: ~164 opportunities (vs 430)                â”‚
â”‚                                                          â”‚
â”‚ 7. âœ¨ MULTICALL BATCH VERIFY (NEW - Phase 2)            â”‚
â”‚    â€¢ Batch up to 20 opportunities                       â”‚
â”‚    â€¢ Single Multicall3 RPC call                         â”‚
â”‚    â€¢ Parse batch results                                â”‚
â”‚    â€¢ 20x faster than sequential                         â”‚
â”‚    â†“ IMPACT: 95% fewer RPC calls                        â”‚
â”‚                                                          â”‚
â”‚ 8. EXECUTE (enhanced)                                   â”‚
â”‚    â€¢ Execute profitable opportunities                   â”‚
â”‚    â€¢ âœ¨ Track slippage delta (NEW)                      â”‚
â”‚    â€¢ Enhanced TradeResult logging                       â”‚
â”‚    â†“                                                     â”‚
â”‚                                                          â”‚
â”‚ 9. âœ¨ UPDATE SCORES (NEW - Phase 1.3)                   â”‚
â”‚    â€¢ Record execution result                            â”‚
â”‚    â€¢ Update pool quality score                          â”‚
â”‚    â€¢ Persist to pool_scores.json                        â”‚
â”‚    â€¢ Self-improving system                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FINAL RESULT:
â”œâ”€ 430 opportunities â†’ 164 opportunities (quality improvement)
â”œâ”€ 860 RPC calls â†’ 10-20 RPC calls (97% reduction)
â”œâ”€ 80% false positives â†’ 18% false positives (77% improvement)
â””â”€ System learns and improves over time
```

---

## ðŸŽ¯ PHASE 1: INTELLIGENT FILTERING

### **Phase 1.1: Static Whitelist/Blacklist** (Days 1-2)

#### **Concept**

```
PURPOSE:
â”œâ”€ Formal validation layer on factory-discovered pools
â”œâ”€ Blacklist known problem pools/tiers
â”œâ”€ Whitelist proven high-quality pools
â”œâ”€ Easy to maintain and update
â””â”€ JSON format (matches existing pattern)

APPROACH:
â”œâ”€ Manual curation (Tier 1 pairs: WMATIC, WETH)
â”œâ”€ Automated discovery (Tier 2 pairs: query subgraph)
â”œâ”€ Historical validation (remove known phantoms)
â””â”€ Hybrid = best of both worlds

BLACKLISTING:
â”œâ”€ Entire fee tiers (1.00% = systematic phantom)
â”œâ”€ Individual pools (AAVE = frozen ticks)
â”œâ”€ Reason documentation (maintainability)
â””â”€ Date tracking (audit trail)
```

#### **File Structure**

```json
{
  "version": "1.0",
  "last_updated": "2026-01-28T00:00:00Z",
  "config": {
    "default_min_liquidity": 1000000000,
    "whitelist_enforcement": "strict"
  },
  "whitelist": {
    "pools": [
      {
        "address": "0x86f1d8390222a3691c28938ec7404a1661e618e0",
        "pair": "WMATIC/USDC",
        "dex": "UniswapV3",
        "fee_tier": 500,
        "status": "active",
        "min_liquidity": 5000000000,
        "tvl_usd": 35000000,
        "notes": "Highest volume, proven reliable",
        "added": "2026-01-28",
        "last_verified": "2026-01-28"
      },
      {
        "address": "0x167384319b41f7094e62f7506409eb38079abff8",
        "pair": "WMATIC/USDC",
        "dex": "UniswapV3",
        "fee_tier": 3000,
        "status": "active",
        "min_liquidity": 3000000000,
        "tvl_usd": 20000000,
        "notes": "Second tier, good volume"
      },
      {
        "address": "0x45dda9cb7c25131df268515131f647d726f50608",
        "pair": "WETH/USDC",
        "dex": "UniswapV3",
        "fee_tier": 500,
        "status": "active",
        "min_liquidity": 5000000000,
        "tvl_usd": 28000000,
        "notes": "High volume ETH pair"
      },
      {
        "address": "0x88f3c15523544835ff6c738ddb30995339ad57d6",
        "pair": "WETH/USDC",
        "dex": "UniswapV3",
        "fee_tier": 3000,
        "status": "active",
        "min_liquidity": 3000000000,
        "tvl_usd": 18000000
      }
    ]
  },
  "blacklist": {
    "pools": [
      {
        "address": "0x...",
        "pair": "AAVE/USDC",
        "dex": "UniswapV3",
        "fee_tier": 500,
        "reason": "Frozen ticks at different price levels",
        "phantom_spread": "69%",
        "date_added": "2026-01-28",
        "discovered_by": "phantom_spread_analysis"
      },
      {
        "address": "0x...",
        "pair": "WETH/USDC",
        "dex": "UniswapV3",
        "fee_tier": 100,
        "reason": "Low liquidity (749B), near-zero executable depth",
        "phantom_spread": "0.78%",
        "date_added": "2026-01-28"
      }
    ],
    "fee_tiers": [
      {
        "tier": 10000,
        "reason": "Systematic phantom liquidity across all 1% pools on Polygon",
        "applies_to": "all_v3",
        "date_added": "2026-01-28",
        "evidence": "phantom_spread_analysis.md"
      }
    ],
    "pairs": []
  },
  "observation": {
    "comment": "Pools under observation - not blacklisted yet, but watch closely",
    "pools": [
      {
        "address": "0x...",
        "pair": "UNI/USDC",
        "fee_tier": 3000,
        "concern": "7.6% spread but price impact exhausts at $140",
        "status": "monitoring",
        "added": "2026-01-28"
      }
    ]
  }
}
```

**Save as**: `config/pools_whitelist.json`

#### **Implementation Code**

```rust
// src/filters/whitelist.rs (NEW FILE)

use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use ethers::types::Address;
use anyhow::{Result, Context};
use std::str::FromStr;

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct PoolWhitelist {
    pub version: String,
    pub last_updated: String,
    pub config: WhitelistConfig,
    pub whitelist: WhitelistSection,
    pub blacklist: BlacklistSection,
    pub observation: ObservationSection,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct WhitelistConfig {
    pub default_min_liquidity: u128,
    pub whitelist_enforcement: String, // "strict" or "advisory"
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct WhitelistSection {
    pub pools: Vec<WhitelistPool>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct WhitelistPool {
    pub address: String,
    pub pair: String,
    pub dex: String,
    pub fee_tier: u32,
    pub status: String,
    pub min_liquidity: u128,
    pub tvl_usd: Option<f64>,
    pub notes: Option<String>,
    pub added: Option<String>,
    pub last_verified: Option<String>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct BlacklistSection {
    pub pools: Vec<BlacklistPool>,
    pub fee_tiers: Vec<BlacklistTier>,
    pub pairs: Vec<BlacklistPair>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct BlacklistPool {
    pub address: String,
    pub pair: String,
    pub dex: String,
    pub fee_tier: u32,
    pub reason: String,
    pub phantom_spread: Option<String>,
    pub date_added: String,
    pub discovered_by: Option<String>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct BlacklistTier {
    pub tier: u32,
    pub reason: String,
    pub applies_to: String,
    pub date_added: String,
    pub evidence: Option<String>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct BlacklistPair {
    pub pair: String,
    pub reason: String,
    pub date_added: String,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ObservationSection {
    pub comment: String,
    pub pools: Vec<ObservationPool>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ObservationPool {
    pub address: String,
    pub pair: String,
    pub fee_tier: u32,
    pub concern: String,
    pub status: String,
    pub added: String,
}

impl PoolWhitelist {
    /// Load whitelist from config file
    pub fn load() -> Result<Self> {
        let path = "config/pools_whitelist.json";
        
        if !std::path::Path::new(path).exists() {
            log::warn!("Whitelist file not found at {}, using empty whitelist", path);
            return Ok(Self::default());
        }
        
        let content = std::fs::read_to_string(path)
            .context("Failed to read whitelist file")?;
        
        let whitelist: PoolWhitelist = serde_json::from_str(&content)
            .context("Failed to parse whitelist JSON")?;
        
        log::info!("Loaded whitelist: {} whitelisted, {} blacklisted pools, {} blacklisted tiers",
                   whitelist.whitelist.pools.len(),
                   whitelist.blacklist.pools.len(),
                   whitelist.blacklist.fee_tiers.len());
        
        Ok(whitelist)
    }
    
    /// Check if a pool is allowed to trade
    pub fn is_pool_allowed(&self, address: &Address, fee_tier: u32, pair: &str) -> bool {
        // Check tier blacklist first (fastest)
        if self.is_tier_blacklisted(fee_tier) {
            log::debug!("Pool {:?} rejected: fee tier {} is blacklisted", address, fee_tier);
            return false;
        }
        
        // Check pool blacklist
        if self.is_pool_blacklisted(address) {
            log::debug!("Pool {:?} rejected: explicitly blacklisted", address);
            return false;
        }
        
        // Check pair blacklist
        if self.is_pair_blacklisted(pair) {
            log::debug!("Pool {:?} rejected: pair {} is blacklisted", address, pair);
            return false;
        }
        
        // If strict mode, must be whitelisted
        if self.config.whitelist_enforcement == "strict" {
            let allowed = self.is_pool_whitelisted(address);
            if !allowed {
                log::debug!("Pool {:?} rejected: not in whitelist (strict mode)", address);
            }
            allowed
        } else {
            // Advisory mode: allow if not blacklisted
            true
        }
    }
    
    /// Check if fee tier is blacklisted
    fn is_tier_blacklisted(&self, fee_tier: u32) -> bool {
        self.blacklist.fee_tiers.iter()
            .any(|t| t.tier == fee_tier)
    }
    
    /// Check if pool is blacklisted
    fn is_pool_blacklisted(&self, address: &Address) -> bool {
        let addr_str = format!("{:?}", address).to_lowercase();
        self.blacklist.pools.iter()
            .any(|p| p.address.to_lowercase() == addr_str)
    }
    
    /// Check if pair is blacklisted
    fn is_pair_blacklisted(&self, pair: &str) -> bool {
        let pair_normalized = pair.to_uppercase();
        self.blacklist.pairs.iter()
            .any(|p| p.pair.to_uppercase() == pair_normalized)
    }
    
    /// Check if pool is whitelisted
    fn is_pool_whitelisted(&self, address: &Address) -> bool {
        let addr_str = format!("{:?}", address).to_lowercase();
        self.whitelist.pools.iter()
            .any(|p| {
                p.address.to_lowercase() == addr_str && 
                p.status == "active"
            })
    }
    
    /// Get minimum liquidity requirement for a pool
    pub fn get_min_liquidity(&self, address: &Address) -> Option<u128> {
        let addr_str = format!("{:?}", address).to_lowercase();
        self.whitelist.pools.iter()
            .find(|p| p.address.to_lowercase() == addr_str)
            .map(|p| p.min_liquidity)
    }
    
    /// Check if pool is under observation
    pub fn is_under_observation(&self, address: &Address) -> bool {
        let addr_str = format!("{:?}", address).to_lowercase();
        self.observation.pools.iter()
            .any(|p| p.address.to_lowercase() == addr_str)
    }
}

impl Default for PoolWhitelist {
    fn default() -> Self {
        Self {
            version: "1.0".to_string(),
            last_updated: chrono::Utc::now().to_rfc3339(),
            config: WhitelistConfig {
                default_min_liquidity: 1_000_000_000,
                whitelist_enforcement: "advisory".to_string(),
            },
            whitelist: WhitelistSection {
                pools: Vec::new(),
            },
            blacklist: BlacklistSection {
                pools: Vec::new(),
                fee_tiers: vec![
                    BlacklistTier {
                        tier: 10000,
                        reason: "Systematic phantom liquidity on Polygon".to_string(),
                        applies_to: "all_v3".to_string(),
                        date_added: chrono::Utc::now().format("%Y-%m-%d").to_string(),
                        evidence: Some("phantom_spread_analysis.md".to_string()),
                    }
                ],
                pairs: Vec::new(),
            },
            observation: ObservationSection {
                comment: "Pools under observation".to_string(),
                pools: Vec::new(),
            },
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_tier_blacklist() {
        let whitelist = PoolWhitelist::default();
        assert!(whitelist.is_tier_blacklisted(10000));
        assert!(!whitelist.is_tier_blacklisted(500));
    }
    
    #[test]
    fn test_whitelist_enforcement() {
        let mut whitelist = PoolWhitelist::default();
        whitelist.config.whitelist_enforcement = "strict".to_string();
        
        let test_addr = Address::from_str("0x0000000000000000000000000000000000000001").unwrap();
        assert!(!whitelist.is_pool_allowed(&test_addr, 500, "TEST/USDC"));
    }
}
```

#### **Integration in detector.rs**

```rust
// In detector.rs - add at the top of your detection loop

use crate::filters::whitelist::PoolWhitelist;

// Load once at startup or cache
let whitelist = PoolWhitelist::load()?;

// In your pool discovery loop, after factory lookup:
for pool in discovered_pools {
    // NEW: Whitelist validation
    if !whitelist.is_pool_allowed(&pool.address, pool.fee_tier, &pool.pair) {
        log::debug!("Pool {:?} filtered by whitelist", pool.address);
        continue;
    }
    
    // Existing filters continue...
    if pool.fee >= 10000 { 
        continue; // This is now redundant (caught by whitelist), but keep for safety
    }
    
    if pool.liquidity < 1000 {
        continue; // Will be enhanced in Phase 1.2
    }
    
    // ... rest of your detection code
}
```

---

### **Phase 1.2: Enhanced Liquidity Thresholds** (Day 2)

#### **Concept**

```
CURRENT PROBLEM:
â”œâ”€ liquidity < 1000 is too low
â”œâ”€ Allows phantom pools through
â”œâ”€ From analysis: 749B liquidity = phantom
â””â”€ Need tier-specific thresholds

SOLUTION:
â”œâ”€ 0.05% tier: >= 5,000,000,000 (5B)
â”œâ”€ 0.30% tier: >= 3,000,000,000 (3B)
â”œâ”€ 1.00% tier: BLACKLISTED (entire tier)
â”œâ”€ V2 pools: >= $500,000 reserves
â””â”€ Configurable per pool in whitelist
```

#### **Implementation**

```rust
// In detector.rs - enhance liquidity check

/// Get minimum liquidity threshold based on fee tier
fn get_min_liquidity_threshold(fee_tier: u32) -> u128 {
    match fee_tier {
        100 => 10_000_000_000,  // 0.01% tier (very tight, needs high liquidity)
        500 => 5_000_000_000,   // 0.05% tier (most common)
        3000 => 3_000_000_000,  // 0.30% tier (standard)
        10000 => u128::MAX,     // 1.00% tier (effectively disabled)
        _ => 1_000_000_000,     // Unknown tiers (conservative)
    }
}

// In your detection loop:
let min_liquidity = whitelist
    .get_min_liquidity(&pool.address)
    .unwrap_or_else(|| get_min_liquidity_threshold(pool.fee_tier));

if pool.liquidity < min_liquidity {
    log::debug!(
        "Pool {:?} liquidity {} below threshold {} (fee tier {})",
        pool.address,
        pool.liquidity,
        min_liquidity,
        pool.fee_tier
    );
    continue;
}
```

#### **Configuration**

Add to `config/pools_whitelist.json`:

```json
{
  "config": {
    "liquidity_thresholds": {
      "v3_100": 10000000000,
      "v3_500": 5000000000,
      "v3_3000": 3000000000,
      "v3_10000": 18446744073709551615,
      "v2_minimum_reserves_usd": 500000
    }
  }
}
```

---

### **Phase 1.3: Pool Quality Scoring** (Days 3-5)

#### **Concept**

```
PURPOSE:
â”œâ”€ Learn from execution history
â”œâ”€ Score pools based on success rate
â”œâ”€ Prioritize high-quality pools
â”œâ”€ Avoid low-quality pools
â””â”€ Self-improving system

SCORING ALGORITHM:
score = (
    0.40 Ã— success_rate +
    0.30 Ã— (1 - slippage_variance) +
    0.20 Ã— recency_factor +
    0.10 Ã— frequency_factor
)

COMPONENTS:
â”œâ”€ success_rate: % of attempts that succeed
â”œâ”€ slippage_variance: avg |actual - quoted| / quoted
â”œâ”€ recency_factor: exponential decay from last success
â””â”€ frequency_factor: executions per day vs target

THRESHOLDS:
â”œâ”€ Score >= 0.7: High quality (prioritize)
â”œâ”€ Score 0.4-0.7: Medium quality (use normally)
â”œâ”€ Score < 0.4: Low quality (skip)
â””â”€ Unknown pools: Start at 0.5 (neutral)

LEARNING:
â”œâ”€ Update after each execution
â”œâ”€ Fast convergence (10-20 attempts)
â”œâ”€ Decay old data (exponential)
â””â”€ Persist to JSON
```

#### **Data Structure**

```json
{
  "version": "1.0",
  "last_updated": "2026-01-28T12:00:00Z",
  "config": {
    "score_weights": {
      "success_rate": 0.40,
      "slippage": 0.30,
      "recency": 0.20,
      "frequency": 0.10
    },
    "thresholds": {
      "high_quality": 0.7,
      "medium_quality": 0.4,
      "default_score": 0.5
    },
    "decay": {
      "recency_halflife_hours": 48,
      "frequency_target_per_day": 5
    }
  },
  "scores": {
    "0x86f1d8390222a3691c28938ec7404a1661e618e0": {
      "pool_address": "0x86f1d8390222a3691c28938ec7404a1661e618e0",
      "pair": "WMATIC/USDC",
      "dex": "UniswapV3",
      "fee_tier": 500,
      "score": 0.87,
      "metrics": {
        "total_attempts": 45,
        "successful_executions": 42,
        "failed_executions": 3,
        "success_rate": 0.93,
        "avg_slippage_variance": 0.03,
        "min_slippage": 0.01,
        "max_slippage": 0.08,
        "last_success": "2026-01-28T11:45:00Z",
        "last_failure": "2026-01-27T18:30:00Z",
        "first_execution": "2026-01-20T00:00:00Z"
      },
      "score_components": {
        "success_rate_score": 0.93,
        "slippage_score": 0.97,
        "recency_score": 1.00,
        "frequency_score": 0.75
      },
      "history_summary": {
        "last_7_days": {
          "attempts": 35,
          "successes": 33,
          "success_rate": 0.94
        },
        "last_24_hours": {
          "attempts": 8,
          "successes": 8,
          "success_rate": 1.00
        }
      }
    }
  }
}
```

**Save as**: `data/pool_scores.json`

#### **Implementation Code**

```rust
// src/scoring/pool_scorer.rs (NEW FILE)

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use chrono::{DateTime, Utc, Duration};
use anyhow::{Result, Context};

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct PoolScores {
    pub version: String,
    pub last_updated: String,
    pub config: ScoringConfig,
    pub scores: HashMap<String, PoolScore>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ScoringConfig {
    pub score_weights: ScoreWeights,
    pub thresholds: ScoreThresholds,
    pub decay: DecayConfig,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ScoreWeights {
    pub success_rate: f64,
    pub slippage: f64,
    pub recency: f64,
    pub frequency: f64,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ScoreThresholds {
    pub high_quality: f64,
    pub medium_quality: f64,
    pub default_score: f64,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct DecayConfig {
    pub recency_halflife_hours: i64,
    pub frequency_target_per_day: f64,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct PoolScore {
    pub pool_address: String,
    pub pair: String,
    pub dex: String,
    pub fee_tier: u32,
    pub score: f64,
    pub metrics: PoolMetrics,
    pub score_components: ScoreComponents,
    pub history_summary: HistorySummary,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct PoolMetrics {
    pub total_attempts: u32,
    pub successful_executions: u32,
    pub failed_executions: u32,
    pub success_rate: f64,
    pub avg_slippage_variance: f64,
    pub min_slippage: f64,
    pub max_slippage: f64,
    pub last_success: Option<String>,
    pub last_failure: Option<String>,
    pub first_execution: Option<String>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ScoreComponents {
    pub success_rate_score: f64,
    pub slippage_score: f64,
    pub recency_score: f64,
    pub frequency_score: f64,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct HistorySummary {
    pub last_7_days: PeriodStats,
    pub last_24_hours: PeriodStats,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct PeriodStats {
    pub attempts: u32,
    pub successes: u32,
    pub success_rate: f64,
}

impl PoolScores {
    /// Load scores from file
    pub fn load() -> Result<Self> {
        let path = "data/pool_scores.json";
        
        if !std::path::Path::new(path).exists() {
            log::info!("Pool scores file not found, creating default");
            return Ok(Self::default());
        }
        
        let content = std::fs::read_to_string(path)
            .context("Failed to read pool scores file")?;
        
        let scores: PoolScores = serde_json::from_str(&content)
            .context("Failed to parse pool scores JSON")?;
        
        log::info!("Loaded {} pool scores", scores.scores.len());
        
        Ok(scores)
    }
    
    /// Save scores to file
    pub fn save(&self) -> Result<()> {
        let path = "data/pool_scores.json";
        
        // Create directory if it doesn't exist
        if let Some(parent) = std::path::Path::new(path).parent() {
            std::fs::create_dir_all(parent)?;
        }
        
        let content = serde_json::to_string_pretty(self)
            .context("Failed to serialize pool scores")?;
        
        std::fs::write(path, content)
            .context("Failed to write pool scores file")?;
        
        Ok(())
    }
    
    /// Get score for a pool (returns default if unknown)
    pub fn get_score(&self, pool_address: &str) -> f64 {
        self.scores
            .get(pool_address)
            .map(|s| s.score)
            .unwrap_or(self.config.thresholds.default_score)
    }
    
    /// Get full pool score data
    pub fn get_pool_score(&self, pool_address: &str) -> Option<&PoolScore> {
        self.scores.get(pool_address)
    }
    
    /// Check if pool meets quality threshold
    pub fn is_quality_pool(&self, pool_address: &str) -> bool {
        let score = self.get_score(pool_address);
        score >= self.config.thresholds.medium_quality
    }
    
    /// Update score after an execution
    pub fn update_after_execution(
        &mut self,
        pool_address: &str,
        pair: &str,
        dex: &str,
        fee_tier: u32,
        success: bool,
        quoted_amount: Option<u128>,
        actual_amount: Option<u128>,
    ) {
        let addr = pool_address.to_string();
        
        // Get or create pool score
        let pool_score = self.scores
            .entry(addr.clone())
            .or_insert_with(|| PoolScore::new(
                pool_address,
                pair,
                dex,
                fee_tier,
                self.config.thresholds.default_score,
            ));
        
        // Update metrics
        pool_score.metrics.total_attempts += 1;
        
        if success {
            pool_score.metrics.successful_executions += 1;
            pool_score.metrics.last_success = Some(Utc::now().to_rfc3339());
        } else {
            pool_score.metrics.failed_executions += 1;
            pool_score.metrics.last_failure = Some(Utc::now().to_rfc3339());
        }
        
        // Set first execution time if not set
        if pool_score.metrics.first_execution.is_none() {
            pool_score.metrics.first_execution = Some(Utc::now().to_rfc3339());
        }
        
        // Calculate and update slippage variance
        if let (Some(quoted), Some(actual)) = (quoted_amount, actual_amount) {
            let slippage = if quoted > 0 {
                ((actual as i128 - quoted as i128).abs() as f64) / (quoted as f64)
            } else {
                0.0
            };
            
            // Update min/max
            pool_score.metrics.min_slippage = pool_score.metrics.min_slippage.min(slippage);
            pool_score.metrics.max_slippage = pool_score.metrics.max_slippage.max(slippage);
            
            // Running average of slippage variance
            let n = pool_score.metrics.total_attempts as f64;
            let old_avg = pool_score.metrics.avg_slippage_variance;
            pool_score.metrics.avg_slippage_variance = 
                (old_avg * (n - 1.0) + slippage) / n;
        }
        
        // Update success rate
        let n = pool_score.metrics.total_attempts as f64;
        pool_score.metrics.success_rate = 
            pool_score.metrics.successful_executions as f64 / n;
        
        // Recalculate score
        pool_score.recalculate_score(&self.config);
        
        // Update timestamp
        self.last_updated = Utc::now().to_rfc3339();
        
        // Persist (async in production)
        if let Err(e) = self.save() {
            log::error!("Failed to save pool scores: {}", e);
        }
        
        log::debug!(
            "Updated score for pool {}: {:.3} (success: {}, attempts: {})",
            pool_address,
            pool_score.score,
            success,
            pool_score.metrics.total_attempts
        );
    }
}

impl PoolScore {
    fn new(address: &str, pair: &str, dex: &str, fee_tier: u32, default_score: f64) -> Self {
        Self {
            pool_address: address.to_string(),
            pair: pair.to_string(),
            dex: dex.to_string(),
            fee_tier,
            score: default_score,
            metrics: PoolMetrics {
                total_attempts: 0,
                successful_executions: 0,
                failed_executions: 0,
                success_rate: 0.0,
                avg_slippage_variance: 0.0,
                min_slippage: f64::MAX,
                max_slippage: 0.0,
                last_success: None,
                last_failure: None,
                first_execution: None,
            },
            score_components: ScoreComponents {
                success_rate_score: default_score,
                slippage_score: default_score,
                recency_score: default_score,
                frequency_score: default_score,
            },
            history_summary: HistorySummary {
                last_7_days: PeriodStats {
                    attempts: 0,
                    successes: 0,
                    success_rate: 0.0,
                },
                last_24_hours: PeriodStats {
                    attempts: 0,
                    successes: 0,
                    success_rate: 0.0,
                },
            },
        }
    }
    
    fn recalculate_score(&mut self, config: &ScoringConfig) {
        let weights = &config.score_weights;
        
        // 1. Success rate component (40%)
        self.score_components.success_rate_score = self.metrics.success_rate;
        
        // 2. Slippage component (30%)
        // Lower variance = higher score
        // Cap variance at 1.0 (100%)
        self.score_components.slippage_score = 
            (1.0 - self.metrics.avg_slippage_variance.min(1.0)).max(0.0);
        
        // 3. Recency component (20%)
        self.score_components.recency_score = 
            self.calculate_recency_score(config.decay.recency_halflife_hours);
        
        // 4. Frequency component (10%)
        self.score_components.frequency_score = 
            self.calculate_frequency_score(config);
        
        // Weighted sum
        self.score = 
            weights.success_rate * self.score_components.success_rate_score +
            weights.slippage * self.score_components.slippage_score +
            weights.recency * self.score_components.recency_score +
            weights.frequency * self.score_components.frequency_score;
        
        // Clamp to [0, 1]
        self.score = self.score.max(0.0).min(1.0);
    }
    
    fn calculate_recency_score(&self, halflife_hours: i64) -> f64 {
        if let Some(last_success_str) = &self.metrics.last_success {
            if let Ok(last_success) = DateTime::parse_from_rfc3339(last_success_str) {
                let hours_since = Utc::now()
                    .signed_duration_since(last_success)
                    .num_hours();
                
                // Exponential decay: 1.0 at t=0, 0.5 at t=halflife
                // score = 2^(-hours/halflife)
                let decay_factor = 2.0_f64.powf(-(hours_since as f64) / (halflife_hours as f64));
                return decay_factor.max(0.0).min(1.0);
            }
        }
        
        // No history or parse error
        if self.metrics.total_attempts == 0 {
            0.5 // Neutral for new pools
        } else {
            0.3 // Penalize pools with no recent success
        }
    }
    
    fn calculate_frequency_score(&self, config: &ScoringConfig) -> f64 {
        if self.metrics.total_attempts < 5 {
            // Not enough data
            return 0.5;
        }
        
        // Calculate executions per day
        if let Some(first_exec_str) = &self.metrics.first_execution {
            if let Ok(first_exec) = DateTime::parse_from_rfc3339(first_exec_str) {
                let days_active = Utc::now()
                    .signed_duration_since(first_exec)
                    .num_days()
                    .max(1) as f64;
                
                let executions_per_day = 
                    self.metrics.total_attempts as f64 / days_active;
                
                // Compare to target
                let target = config.decay.frequency_target_per_day;
                let ratio = executions_per_day / target;
                
                // Score = min(1.0, ratio)
                // If ratio >= 1.0, pool meets or exceeds target
                return ratio.min(1.0);
            }
        }
        
        // Fallback: use attempt count as proxy
        match self.metrics.total_attempts {
            0..=5 => 0.3,
            6..=20 => 0.6,
            21..=50 => 0.8,
            _ => 1.0,
        }
    }
}

impl Default for PoolScores {
    fn default() -> Self {
        Self {
            version: "1.0".to_string(),
            last_updated: Utc::now().to_rfc3339(),
            config: ScoringConfig {
                score_weights: ScoreWeights {
                    success_rate: 0.40,
                    slippage: 0.30,
                    recency: 0.20,
                    frequency: 0.10,
                },
                thresholds: ScoreThresholds {
                    high_quality: 0.7,
                    medium_quality: 0.4,
                    default_score: 0.5,
                },
                decay: DecayConfig {
                    recency_halflife_hours: 48,
                    frequency_target_per_day: 5.0,
                },
            },
            scores: HashMap::new(),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_new_pool_score() {
        let score = PoolScore::new(
            "0x1234",
            "WMATIC/USDC",
            "UniswapV3",
            500,
            0.5,
        );
        assert_eq!(score.score, 0.5);
        assert_eq!(score.metrics.total_attempts, 0);
    }
    
    #[test]
    fn test_score_update() {
        let mut scores = PoolScores::default();
        
        // First execution: success
        scores.update_after_execution(
            "0x1234",
            "TEST/USDC",
            "UniswapV3",
            500,
            true,
            Some(1000),
            Some(980), // 2% slippage
        );
        
        let score = scores.get_score("0x1234");
        assert!(score > 0.5); // Should be above default
        
        // Second execution: success
        scores.update_after_execution(
            "0x1234",
            "TEST/USDC",
            "UniswapV3",
            500,
            true,
            Some(1000),
            Some(985), // 1.5% slippage
        );
        
        let new_score = scores.get_score("0x1234");
        assert!(new_score > score); // Should improve
    }
}
```

#### **Integration in detector.rs**

```rust
// In detector.rs - load at startup or cache

use crate::scoring::pool_scorer::PoolScores;

// Load scores (cache this, don't reload every iteration)
let pool_scores = Arc::new(Mutex::new(PoolScores::load()?));

// In detection loop, after whitelist and liquidity checks:
{
    let scores = pool_scores.lock().await;
    let score = scores.get_score(&pool.address.to_string());
    
    if score < 0.4 {
        log::debug!(
            "Pool {:?} score {:.3} below threshold, skipping",
            pool.address,
            score
        );
        continue;
    }
    
    // Optional: Log high-quality pools
    if score >= 0.7 {
        log::debug!("High-quality pool detected: {:?} (score: {:.3})", pool.address, score);
    }
}

// Continue with detection...
```

#### **Integration in executor.rs**

```rust
// In executor.rs - after trade execution

use crate::scoring::pool_scorer::PoolScores;

// After executing a trade
async fn update_pool_score_after_trade(
    pool_scores: Arc<Mutex<PoolScores>>,
    pool_address: &Address,
    pair: &str,
    dex: &str,
    fee_tier: u32,
    trade_result: &TradeResult,
    quoted_amount: Option<U256>,
) {
    let mut scores = pool_scores.lock().await;
    
    scores.update_after_execution(
        &format!("{:?}", pool_address),
        pair,
        dex,
        fee_tier,
        trade_result.success,
        quoted_amount.map(|a| a.as_u128()),
        Some(trade_result.actual_amount_out.as_u128()),
    );
}

// Call after each execution:
update_pool_score_after_trade(
    pool_scores.clone(),
    &pool_address,
    &pair,
    &dex,
    fee_tier,
    &trade_result,
    quoted_amount,
).await;
```

#### **Enhanced TradeResult for Slippage Tracking**

```rust
// In executor.rs or wherever TradeResult is defined

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct TradeResult {
    // ... existing fields ...
    
    // NEW FIELDS for slippage tracking:
    pub quoted_amount_in: Option<U256>,
    pub quoted_amount_out: Option<U256>,
    pub actual_amount_in: U256,
    pub actual_amount_out: U256,
    pub slippage_pct: Option<f64>,
    pub slippage_exceeded_threshold: bool,
}

impl TradeResult {
    /// Calculate slippage percentage
    pub fn calculate_slippage(&mut self) {
        if let (Some(quoted_out), actual_out) = (self.quoted_amount_out, self.actual_amount_out) {
            let quoted = quoted_out.as_u128() as f64;
            let actual = actual_out.as_u128() as f64;
            
            if quoted > 0.0 {
                // Slippage = (actual - quoted) / quoted Ã— 100
                // Negative slippage = worse than expected
                self.slippage_pct = Some((actual - quoted) / quoted * 100.0);
                
                // Check if exceeded threshold (e.g., >5%)
                if let Some(slippage) = self.slippage_pct {
                    self.slippage_exceeded_threshold = slippage.abs() > 5.0;
                }
            }
        }
    }
}
```

---

## ðŸŽ¯ PHASE 2: MULTICALL BATCHING

### **Phase 2.1: Multicall3 Integration** (Days 6-8)

#### **Concept**

```
PROBLEM:
â”œâ”€ Sequential Quoter calls = 1 RPC per opportunity
â”œâ”€ 164 opportunities (after Phase 1) = 328 RPC calls
â”œâ”€ Latency: ~200ms per call Ã— 2 = 400ms per opportunity
â””â”€ Rate limiting risk

SOLUTION:
â”œâ”€ Batch multiple Quoter calls into single Multicall3
â”œâ”€ 1 RPC call for 20 opportunities (40 Quoter calls)
â”œâ”€ Latency: ~100ms for entire batch
â””â”€ 95% reduction in RPC calls

MULTICALL3:
â”œâ”€ Deployed on Polygon: 0xcA11bde05977b3631167028862bE2a173976CA11
â”œâ”€ Standard contract, widely used
â”œâ”€ Supports failure handling (allowFailure)
â””â”€ Returns all results in single call
```

#### **Dependencies**

```toml
# Add to Cargo.toml
[dependencies]
ethers = { version = "2.0", features = ["abigen", "ws"] }
tokio = { version = "1", features = ["full"] }
anyhow = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
log = "0.4"
```

#### **Implementation Code**

```rust
// src/verification/multicall_verifier.rs (NEW FILE)

use ethers::prelude::*;
use ethers::abi::{encode, Token, ParamType};
use anyhow::{Result, Context};
use std::sync::Arc;

/// Multicall3 is deployed at this address on all chains
const MULTICALL3_ADDRESS: &str = "0xcA11bde05977b3631167028862bE2a173976CA11";

/// Uniswap V3 Quoter V2
const QUOTER_V2_ADDRESS: &str = "0xb27308f9F90D607463bb33eA1BeBb41C27CE5AB6";

pub struct MulticallVerifier {
    provider: Arc<Provider<Ws>>,
    multicall_address: Address,
    quoter_address: Address,
}

#[derive(Debug, Clone)]
pub struct OpportunityToVerify {
    pub id: String,
    pub token_in: Address,
    pub token_mid: Address,
    pub token_out: Address,
    pub amount_in: U256,
    pub fee_tier_1: u32,
    pub fee_tier_2: u32,
    pub pool_1: Address,
    pub pool_2: Address,
    pub estimated_spread: f64,
}

#[derive(Debug, Clone)]
pub struct VerifiedOpportunity {
    pub opportunity: OpportunityToVerify,
    pub quoted_mid: U256,
    pub quoted_out: U256,
    pub estimated_profit_usd: f64,
    pub verified: bool,
    pub verification_error: Option<String>,
}

impl MulticallVerifier {
    pub fn new(provider: Arc<Provider<Ws>>) -> Result<Self> {
        let multicall_address = MULTICALL3_ADDRESS
            .parse()
            .context("Invalid Multicall3 address")?;
        
        let quoter_address = QUOTER_V2_ADDRESS
            .parse()
            .context("Invalid Quoter address")?;
        
        Ok(Self {
            provider,
            multicall_address,
            quoter_address,
        })
    }
    
    /// Batch verify multiple opportunities with a single RPC call
    pub async fn batch_quote_opportunities(
        &self,
        opportunities: Vec<OpportunityToVerify>,
    ) -> Result<Vec<VerifiedOpportunity>> {
        
        if opportunities.is_empty() {
            return Ok(Vec::new());
        }
        
        log::info!("Batching {} opportunities for verification", opportunities.len());
        
        // Build calls array for Multicall3
        let mut calls = Vec::new();
        
        for opp in &opportunities {
            // Call 1: Quote leg 1 (token_in -> token_mid)
            let call1 = self.build_quote_call(
                opp.token_in,
                opp.token_mid,
                opp.amount_in,
                opp.fee_tier_1,
            )?;
            calls.push(call1);
            
            // Call 2: Quote leg 2 (token_mid -> token_out)
            // Use estimated mid amount (will be refined with actual if needed)
            let estimated_mid = self.estimate_mid_amount(opp);
            let call2 = self.build_quote_call(
                opp.token_mid,
                opp.token_out,
                estimated_mid,
                opp.fee_tier_2,
            )?;
            calls.push(call2);
        }
        
        // Execute multicall (SINGLE RPC CALL!)
        let results = self.execute_multicall(calls).await?;
        
        // Parse results
        let verified = self.parse_batch_results(opportunities, results)?;
        
        log::info!(
            "Verified batch: {} opportunities, {} profitable",
            verified.len(),
            verified.iter().filter(|v| v.verified).count()
        );
        
        Ok(verified)
    }
    
    /// Build a Quoter call for Multicall3
    fn build_quote_call(
        &self,
        token_in: Address,
        token_out: Address,
        amount_in: U256,
        fee_tier: u32,
    ) -> Result<Call3> {
        // Quoter function: quoteExactInputSingle((address,address,uint256,uint24,uint160))
        // Returns: (uint256 amountOut, uint160 sqrtPriceX96After, uint32 initializedTicksCrossed, uint256 gasEstimate)
        
        let function_selector = &ethers::utils::id(
            "quoteExactInputSingle((address,address,uint256,uint24,uint160))"
        )[..4];
        
        // Encode parameters as tuple
        let params = encode(&[Token::Tuple(vec![
            Token::Address(token_in),
            Token::Address(token_out),
            Token::Uint(amount_in),
            Token::Uint(U256::from(fee_tier)),
            Token::Uint(U256::zero()), // sqrtPriceLimitX96 = 0 (no limit)
        ])]);
        
        let mut call_data = function_selector.to_vec();
        call_data.extend_from_slice(&params);
        
        Ok(Call3 {
            target: self.quoter_address,
            allow_failure: true, // Don't revert entire batch if one call fails
            call_data: Bytes::from(call_data),
        })
    }
    
    /// Execute Multicall3.aggregate3 call
    async fn execute_multicall(&self, calls: Vec<Call3>) -> Result<Vec<Result3>> {
        // Multicall3.aggregate3(Call3[] calldata calls) returns (Result3[] memory returnData)
        
        let function_selector = &ethers::utils::id("aggregate3((address,bool,bytes)[])"
        )[..4];
        
        // Encode calls array
        let tokens: Vec<Token> = calls.iter().map(|call| {
            Token::Tuple(vec![
                Token::Address(call.target),
                Token::Bool(call.allow_failure),
                Token::Bytes(call.call_data.to_vec()),
            ])
        }).collect();
        
        let params = encode(&[Token::Array(tokens)]);
        
        let mut call_data = function_selector.to_vec();
        call_data.extend_from_slice(&params);
        
        // Make the call
        let tx = TransactionRequest::new()
            .to(self.multicall_address)
            .data(call_data);
        
        let result = self.provider
            .call(&tx.into(), None)
            .await
            .context("Multicall3 execution failed")?;
        
        // Decode results
        let decoded = ethers::abi::decode(
            &[ParamType::Array(Box::new(ParamType::Tuple(vec![
                ParamType::Bool,  // success
                ParamType::Bytes, // returnData
            ])))],
            &result,
        )?;
        
        let results = if let Some(Token::Array(arr)) = decoded.get(0) {
            arr.iter().map(|token| {
                if let Token::Tuple(tuple) = token {
                    let success = if let Some(Token::Bool(s)) = tuple.get(0) {
                        *s
                    } else {
                        false
                    };
                    
                    let return_data = if let Some(Token::Bytes(data)) = tuple.get(1) {
                        Bytes::from(data.clone())
                    } else {
                        Bytes::new()
                    };
                    
                    Result3 { success, return_data }
                } else {
                    Result3 {
                        success: false,
                        return_data: Bytes::new(),
                    }
                }
            }).collect()
        } else {
            Vec::new()
        };
        
        Ok(results)
    }
    
    /// Parse batch results and match to opportunities
    fn parse_batch_results(
        &self,
        opportunities: Vec<OpportunityToVerify>,
        results: Vec<Result3>,
    ) -> Result<Vec<VerifiedOpportunity>> {
        
        let mut verified = Vec::new();
        
        // Results come in pairs: (leg1_quote, leg2_quote) for each opportunity
        for (idx, opp) in opportunities.iter().enumerate() {
            let leg1_idx = idx * 2;
            let leg2_idx = idx * 2 + 1;
            
            if leg1_idx >= results.len() || leg2_idx >= results.len() {
                // Incomplete results
                verified.push(VerifiedOpportunity {
                    opportunity: opp.clone(),
                    quoted_mid: U256::zero(),
                    quoted_out: U256::zero(),
                    estimated_profit_usd: 0.0,
                    verified: false,
                    verification_error: Some("Incomplete results".to_string()),
                });
                continue;
            }
            
            let leg1_result = &results[leg1_idx];
            let leg2_result = &results[leg2_idx];
            
            // Check if both legs succeeded
            if !leg1_result.success || !leg2_result.success {
                verified.push(VerifiedOpportunity {
                    opportunity: opp.clone(),
                    quoted_mid: U256::zero(),
                    quoted_out: U256::zero(),
                    estimated_profit_usd: 0.0,
                    verified: false,
                    verification_error: Some("Quoter call failed".to_string()),
                });
                continue;
            }
            
            // Decode leg 1 result
            let amount_mid = match self.decode_quote_result(&leg1_result.return_data) {
                Ok(amount) => amount,
                Err(e) => {
                    verified.push(VerifiedOpportunity {
                        opportunity: opp.clone(),
                        quoted_mid: U256::zero(),
                        quoted_out: U256::zero(),
                        estimated_profit_usd: 0.0,
                        verified: false,
                        verification_error: Some(format!("Leg 1 decode failed: {}", e)),
                    });
                    continue;
                }
            };
            
            // Decode leg 2 result
            let amount_out = match self.decode_quote_result(&leg2_result.return_data) {
                Ok(amount) => amount,
                Err(e) => {
                    verified.push(VerifiedOpportunity {
                        opportunity: opp.clone(),
                        quoted_mid: amount_mid,
                        quoted_out: U256::zero(),
                        estimated_profit_usd: 0.0,
                        verified: false,
                        verification_error: Some(format!("Leg 2 decode failed: {}", e)),
                    });
                    continue;
                }
            };
            
            // Check profitability
            let profitable = amount_out > opp.amount_in;
            
            let profit_usd = if profitable {
                let profit_tokens = amount_out - opp.amount_in;
                // Convert to USD (assuming USDC with 6 decimals)
                profit_tokens.as_u128() as f64 / 1_000_000.0
            } else {
                0.0
            };
            
            verified.push(VerifiedOpportunity {
                opportunity: opp.clone(),
                quoted_mid: amount_mid,
                quoted_out: amount_out,
                estimated_profit_usd: profit_usd,
                verified: profitable,
                verification_error: None,
            });
        }
        
        Ok(verified)
    }
    
    /// Decode Quoter result
    fn decode_quote_result(&self, data: &Bytes) -> Result<U256> {
        // Quoter returns: (uint256 amountOut, uint160 sqrtPriceX96After, uint32 initializedTicksCrossed, uint256 gasEstimate)
        // We only need the first value (amountOut)
        
        if data.len() < 32 {
            return Err(anyhow::anyhow!("Invalid Quoter response length"));
        }
        
        let amount = U256::from_big_endian(&data[..32]);
        Ok(amount)
    }
    
    /// Estimate mid amount for initial quote
    fn estimate_mid_amount(&self, opp: &OpportunityToVerify) -> U256 {
        // Conservative estimate: assume 95% of input (5% spread/fees)
        opp.amount_in * 95 / 100
    }
}

#[derive(Debug, Clone)]
struct Call3 {
    target: Address,
    allow_failure: bool,
    call_data: Bytes,
}

#[derive(Debug, Clone)]
struct Result3 {
    success: bool,
    return_data: Bytes,
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_estimate_mid_amount() {
        let verifier = MulticallVerifier {
            provider: Arc::new(Provider::try_from("http://localhost:8545").unwrap()),
            multicall_address: MULTICALL3_ADDRESS.parse().unwrap(),
            quoter_address: QUOTER_V2_ADDRESS.parse().unwrap(),
        };
        
        let opp = OpportunityToVerify {
            id: "test".to_string(),
            token_in: Address::zero(),
            token_mid: Address::zero(),
            token_out: Address::zero(),
            amount_in: U256::from(1000),
            fee_tier_1: 500,
            fee_tier_2: 500,
            pool_1: Address::zero(),
            pool_2: Address::zero(),
            estimated_spread: 2.0,
        };
        
        let mid = verifier.estimate_mid_amount(&opp);
        assert_eq!(mid, U256::from(950)); // 95% of 1000
    }
}
```

#### **Integration in executor.rs**

```rust
// In executor.rs - replace sequential Quoter calls

use crate::verification::multicall_verifier::{MulticallVerifier, OpportunityToVerify};

// At startup:
let multicall_verifier = Arc::new(
    MulticallVerifier::new(provider.clone())?
);

// Replace your sequential verification loop:

// OLD CODE (commented out):
// for opp in opportunities {
//     let quote1 = quoter.quote(opp.leg1).await?;  // RPC call 1
//     let quote2 = quoter.quote(opp.leg2).await?;  // RPC call 2
//     if profitable { execute(opp).await? }
// }

// NEW CODE:
// 1. Convert detected opportunities to verification format
let opportunities_to_verify: Vec<OpportunityToVerify> = 
    opportunities.iter().map(|opp| {
        OpportunityToVerify {
            id: opp.id.clone(),
            token_in: opp.token_in,
            token_mid: opp.token_mid,
            token_out: opp.token_out,
            amount_in: opp.amount_in,
            fee_tier_1: opp.fee_tier_1,
            fee_tier_2: opp.fee_tier_2,
            pool_1: opp.pool_1,
            pool_2: opp.pool_2,
            estimated_spread: opp.spread_pct,
        }
    }).collect();

// 2. Batch verify (SINGLE RPC CALL!)
let verified = multicall_verifier
    .batch_quote_opportunities(opportunities_to_verify)
    .await?;

// 3. Execute profitable opportunities
for verified_opp in verified {
    if !verified_opp.verified {
        continue;
    }
    
    if verified_opp.estimated_profit_usd < min_profit_usd {
        continue;
    }
    
    // Execute trade
    match execute_trade(verified_opp).await {
        Ok(trade_result) => {
            log::info!("Trade executed: profit ${:.2}", trade_result.profit_usd);
            
            // Update pool score
            update_pool_score_after_trade(
                pool_scores.clone(),
                &verified_opp.opportunity.pool_1,
                &pair,
                &dex,
                verified_opp.opportunity.fee_tier_1,
                &trade_result,
                Some(verified_opp.quoted_mid),
            ).await;
        }
        Err(e) => {
            log::error!("Trade execution failed: {}", e);
        }
    }
}
```

---

### **Phase 2.2: Adaptive Batch Management** (Days 9-10)

#### **Concept**

```
PROBLEM:
â”œâ”€ Opportunities don't arrive at once
â”œâ”€ May get 1 opportunity, then wait, then get 10 more
â”œâ”€ Should we verify immediately or wait to batch?
â””â”€ Trade-off: latency vs efficiency

SOLUTIONS:

1. TIME-BASED: Wait max 500ms, verify whatever we have
2. SIZE-BASED: Wait until batch reaches 20, then verify
3. HYBRID (BEST): Whichever comes first

ADAPTIVE LOGIC:
â”œâ”€ High opportunity flow (>10/sec): Use size-based (20)
â”œâ”€ Medium flow (1-10/sec): Hybrid (20 or 500ms)
â”œâ”€ Low flow (<1/sec): Time-based (500ms)
â””â”€ Adjusts automatically based on flow rate
```

#### **Implementation**

```rust
// src/verification/batch_manager.rs (NEW FILE)

use tokio::sync::Mutex;
use tokio::time::{Duration, sleep, Instant};
use std::sync::Arc;
use crate::verification::multicall_verifier::{MulticallVerifier, OpportunityToVerify, VerifiedOpportunity};
use anyhow::Result;

pub struct BatchManager {
    pending: Arc<Mutex<Vec<OpportunityToVerify>>>,
    verifier: Arc<MulticallVerifier>,
    config: BatchConfig,
    last_flush: Arc<Mutex<Instant>>,
}

#[derive(Debug, Clone)]
pub struct BatchConfig {
    pub max_batch_size: usize,
    pub max_wait_ms: u64,
    pub min_batch_size: usize,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 20,
            max_wait_ms: 500,
            min_batch_size: 1,
        }
    }
}

impl BatchManager {
    pub fn new(verifier: Arc<MulticallVerifier>, config: BatchConfig) -> Self {
        Self {
            pending: Arc::new(Mutex::new(Vec::new())),
            verifier,
            config,
            last_flush: Arc::new(Mutex::new(Instant::now())),
        }
    }
    
    /// Add an opportunity to the batch queue
    pub async fn add_opportunity(&self, opp: OpportunityToVerify) -> Option<Vec<VerifiedOpportunity>> {
        let mut pending = self.pending.lock().await;
        pending.push(opp);
        
        // Check if batch is full
        if pending.len() >= self.config.max_batch_size {
            log::debug!("Batch full ({} opportunities), flushing", pending.len());
            drop(pending); // Release lock before flushing
            return Some(self.flush_batch().await.unwrap_or_default());
        }
        
        None
    }
    
    /// Start background flusher task
    pub async fn start_background_flusher(self: Arc<Self>) {
        tokio::spawn(async move {
            loop {
                sleep(Duration::from_millis(self.config.max_wait_ms)).await;
                
                // Check if we should flush
                let should_flush = {
                    let pending = self.pending.lock().await;
                    let last_flush = self.last_flush.lock().await;
                    
                    !pending.is_empty() && 
                    last_flush.elapsed() >= Duration::from_millis(self.config.max_wait_ms)
                };
                
                if should_flush {
                    log::debug!("Timeout reached, flushing batch");
                    if let Err(e) = self.flush_batch().await {
                        log::error!("Background flush failed: {}", e);
                    }
                }
            }
        });
    }
    
    /// Manually flush the batch
    pub async fn flush_batch(&self) -> Result<Vec<VerifiedOpportunity>> {
        // Extract pending opportunities
        let batch = {
            let mut pending = self.pending.lock().await;
            
            if pending.is_empty() {
                return Ok(Vec::new());
            }
            
            if pending.len() < self.config.min_batch_size {
                log::debug!("Batch too small ({} opportunities), waiting", pending.len());
                return Ok(Vec::new());
            }
            
            pending.drain(..).collect::<Vec<_>>()
        };
        
        log::info!("Flushing batch of {} opportunities", batch.len());
        
        // Update last flush time
        {
            let mut last_flush = self.last_flush.lock().await;
            *last_flush = Instant::now();
        }
        
        // Verify batch
        let verified = self.verifier.batch_quote_opportunities(batch).await?;
        
        log::info!(
            "Batch verified: {} opportunities, {} profitable",
            verified.len(),
            verified.iter().filter(|v| v.verified).count()
        );
        
        Ok(verified)
    }
    
    /// Get current batch size
    pub async fn pending_count(&self) -> usize {
        self.pending.lock().await.len()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ethers::types::Address;
    
    #[tokio::test]
    async fn test_batch_size_trigger() {
        // This test would require a mock verifier
        // Omitted for brevity
    }
}
```

#### **Integration with Main Loop**

```rust
// In main.rs or your main detection loop:

use crate::verification::batch_manager::{BatchManager, BatchConfig};
use crate::verification::multicall_verifier::MulticallVerifier;

#[tokio::main]
async fn main() -> Result<()> {
    // ... existing setup ...
    
    // Create multicall verifier
    let multicall_verifier = Arc::new(
        MulticallVerifier::new(provider.clone())?
    );
    
    // Create batch manager
    let batch_config = BatchConfig {
        max_batch_size: 20,
        max_wait_ms: 500,
        min_batch_size: 1,
    };
    
    let batch_manager = Arc::new(
        BatchManager::new(multicall_verifier.clone(), batch_config)
    );
    
    // Start background flusher
    batch_manager.clone().start_background_flusher().await;
    
    // ... detection loop ...
    
    loop {
        // Detect opportunities
        let opportunities = detect_opportunities().await?;
        
        for opp in opportunities {
            // Add to batch queue
            if let Some(verified) = batch_manager.add_opportunity(opp).await {
                // Batch was flushed, execute profitable opportunities
                for verified_opp in verified {
                    if verified_opp.verified {
                        execute_trade(verified_opp).await?;
                    }
                }
            }
        }
        
        // Small delay
        sleep(Duration::from_millis(100)).await;
    }
}
```

---

## ðŸ“Š EXPECTED IMPACT ANALYSIS

### **Phase 1 Impact Breakdown**

```
BASELINE (Current):
â”œâ”€ Opportunities detected: 430/hour
â”œâ”€ False positives: ~80% (344 phantom)
â”œâ”€ Quoter calls: 860/hour (430 Ã— 2 legs)
â”œâ”€ Latency: ~200ms per check
â””â”€ Executable: ~86/hour (20%)

AFTER WHITELIST (Phase 1.1):
â”œâ”€ Blacklist 1.00% tier: -100 opportunities
â”œâ”€ Blacklist AAVE pools: -20 opportunities
â”œâ”€ Whitelist enforcement: -50 unknown pools
â”œâ”€ Total filtered: -170 opportunities (-40%)
â”œâ”€ Remaining: 260/hour
â”œâ”€ False positive rate: ~60% (improvements not linear)
â””â”€ Quoter calls: 520/hour (-40%)

AFTER ENHANCED LIQUIDITY (Phase 1.2):
â”œâ”€ Filter <5B on 0.05%: -40 opportunities
â”œâ”€ Filter <3B on 0.30%: -30 opportunities
â”œâ”€ Total filtered: -70 more (-27% from 260)
â”œâ”€ Remaining: 190/hour
â”œâ”€ False positive rate: ~30%
â””â”€ Quoter calls: 380/hour (-56% from baseline)

AFTER POOL SCORING (Phase 1.3):
â”œâ”€ Filter score <0.4: -26 opportunities
â”œâ”€ Total filtered: -26 more (-14% from 190)
â”œâ”€ Remaining: 164/hour
â”œâ”€ False positive rate: ~18% (95 profitable, 69 phantom)
â””â”€ Quoter calls: 328/hour (-62% from baseline)

PHASE 1 TOTAL IMPACT:
â”œâ”€ Opportunities: 430 â†’ 164 (-62%)
â”œâ”€ False positives: 80% â†’ 18% (-77%)
â”œâ”€ Quoter calls: 860 â†’ 328 (-62%)
â”œâ”€ Quality: Much higher (more executable)
â””â”€ Self-improving (scoring learns)
```

### **Phase 2 Impact Breakdown**

```
AFTER PHASE 1: 328 Quoter calls/hour

WITH MULTICALL BATCHING:
â”œâ”€ Batch size: 20 opportunities
â”œâ”€ 164 opportunities / 20 = ~8 batches
â”œâ”€ Each batch = 1 Multicall RPC call
â”œâ”€ 8 batches + retries = ~12 RPC calls/hour
â”œâ”€ Reduction: 328 â†’ 12 (-96%)

WITH ADAPTIVE BATCHING:
â”œâ”€ Optimize batch sizes dynamically
â”œâ”€ Reduce wasted time on small batches
â”œâ”€ Better latency for urgent opportunities
â”œâ”€ Expected: 10-15 RPC calls/hour
â””â”€ Reduction: 328 â†’ 10-15 (-95-97%)

LATENCY IMPROVEMENT:
â”œâ”€ Before: 200ms per opportunity Ã— 2 legs = 400ms
â”œâ”€ After: 100ms per batch / 20 opportunities = 5ms per opportunity
â””â”€ Reduction: 400ms â†’ 5ms (-99% per opportunity)

PHASE 2 TOTAL IMPACT:
â”œâ”€ RPC calls: 328 â†’ 10-15 (-95-97%)
â”œâ”€ Latency: 400ms â†’ 5ms per opportunity (-99%)
â”œâ”€ Can check MORE opportunities in same time
â””â”€ Lower rate limiting risk
```

### **Combined Phase 1 & 2 Impact**

```
BASELINE â†’ FINAL:

Opportunities:
â”œâ”€ Before: 430/hour (80% phantom)
â”œâ”€ After: 164/hour (18% phantom)
â””â”€ Improvement: Better quality, -62% noise

RPC Calls:
â”œâ”€ Before: 860/hour
â”œâ”€ After: 10-15/hour
â””â”€ Improvement: -97-98%

Latency per Opportunity:
â”œâ”€ Before: 400ms (sequential quotes)
â”œâ”€ After: 5ms (batched)
â””â”€ Improvement: -99%

False Positive Rate:
â”œâ”€ Before: 80% (344 phantom out of 430)
â”œâ”€ After: 18% (30 phantom out of 164)
â””â”€ Improvement: -77%

Executable Opportunities:
â”œâ”€ Before: 86/hour (20%)
â”œâ”€ After: 134/hour (82%)
â””â”€ Improvement: +56% absolute executable

System Characteristics:
â”œâ”€ Self-improving (pool scoring learns)
â”œâ”€ Maintains quality whitelist
â”œâ”€ Efficient RPC usage
â””â”€ Sub-10ms latency per check
```

---

## ðŸ“… IMPLEMENTATION TIMELINE

### **Week 1: Phase 1**

```
MONDAY (Day 1):
â”œâ”€ 9:00-11:00: Create pools_whitelist.json structure
â”œâ”€ 11:00-13:00: Implement whitelist.rs
â”œâ”€ 14:00-16:00: Generate initial whitelist (query Uniswap subgraph)
â”œâ”€ 16:00-18:00: Integrate in detector.rs
â””â”€ 18:00-19:00: Test in paper trading

TUESDAY (Day 2):
â”œâ”€ 9:00-11:00: Implement enhanced liquidity thresholds
â”œâ”€ 11:00-13:00: Test combined whitelist + liquidity
â”œâ”€ 14:00-16:00: Measure impact on opportunity count
â”œâ”€ 16:00-18:00: Document and commit
â””â”€ 18:00-19:00: Update configuration files

WEDNESDAY (Day 3):
â”œâ”€ 9:00-12:00: Implement pool_scorer.rs (scoring logic)
â”œâ”€ 12:00-14:00: Implement pool_scores.json persistence
â”œâ”€ 14:00-16:00: Add slippage delta tracking to TradeResult
â”œâ”€ 16:00-18:00: Unit tests for scoring
â””â”€ 18:00-19:00: Integration testing

THURSDAY (Day 4):
â”œâ”€ 9:00-11:00: Integrate scoring in detector.rs
â”œâ”€ 11:00-13:00: Integrate scoring updates in executor.rs
â”œâ”€ 14:00-16:00: Test end-to-end scoring flow
â”œâ”€ 16:00-18:00: Fix bugs, refine thresholds
â””â”€ 18:00-19:00: Documentation

FRIDAY (Day 5):
â”œâ”€ 9:00-12:00: Full integration testing (all Phase 1 components)
â”œâ”€ 12:00-14:00: Paper trading with all filters
â”œâ”€ 14:00-16:00: Monitor and tune thresholds
â”œâ”€ 16:00-18:00: Performance analysis and report
â””â”€ 18:00-19:00: Week 1 retrospective

WEEKEND:
â”œâ”€ Saturday: Continue paper trading, collect data
â”œâ”€ Sunday: Review metrics, prepare for Phase 2
â””â”€ Optional: Start reading Multicall3 documentation
```

### **Week 2: Phase 2**

```
MONDAY (Day 6):
â”œâ”€ 9:00-11:00: Implement multicall_verifier.rs structure
â”œâ”€ 11:00-13:00: Implement call building logic
â”œâ”€ 14:00-16:00: Implement result parsing
â”œâ”€ 16:00-18:00: Unit tests
â””â”€ 18:00-19:00: Test Multicall3 calls (simple cases)

TUESDAY (Day 7):
â”œâ”€ 9:00-11:00: Integrate MulticallVerifier in executor.rs
â”œâ”€ 11:00-13:00: Replace sequential Quoter calls
â”œâ”€ 14:00-16:00: Test in paper trading
â”œâ”€ 16:00-18:00: Compare results with sequential (accuracy check)
â””â”€ 18:00-19:00: Fix any bugs

WEDNESDAY (Day 8):
â”œâ”€ 9:00-12:00: Implement batch_manager.rs
â”œâ”€ 12:00-14:00: Add adaptive batching logic
â”œâ”€ 14:00-16:00: Test batching with various flows
â”œâ”€ 16:00-18:00: Integrate in main loop
â””â”€ 18:00-19:00: Integration testing

THURSDAY (Day 9):
â”œâ”€ 9:00-11:00: Performance testing and measurement
â”œâ”€ 11:00-13:00: RPC call counting (verify 95% reduction)
â”œâ”€ 14:00-16:00: Latency measurement
â”œâ”€ 16:00-18:00: Tune batch parameters (size, timeout)
â””â”€ 18:00-19:00: Optimization

FRIDAY (Day 10):
â”œâ”€ 9:00-12:00: Full integration test (Phase 1 + Phase 2)
â”œâ”€ 12:00-14:00: Extended paper trading session
â”œâ”€ 14:00-16:00: Performance report and metrics
â”œâ”€ 16:00-18:00: Documentation and cleanup
â””â”€ 18:00-19:00: Week 2 retrospective

WEEKEND:
â”œâ”€ Saturday: Extended paper trading (24 hours)
â”œâ”€ Sunday: Final validation, prepare for live deployment
â””â”€ Optional: Start planning V4 (multi-pair expansion)
```

---

## ðŸ§ª TESTING STRATEGY

### **Phase 1 Testing**

#### **Unit Tests**

```rust
// tests/whitelist_tests.rs

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_whitelist_loading() {
        let whitelist = PoolWhitelist::load().unwrap();
        assert!(whitelist.whitelist.pools.len() > 0);
    }
    
    #[test]
    fn test_tier_blacklist() {
        let whitelist = PoolWhitelist::default();
        assert!(whitelist.is_tier_blacklisted(10000));
        assert!(!whitelist.is_tier_blacklisted(500));
    }
    
    #[test]
    fn test_pool_blacklist() {
        let mut whitelist = PoolWhitelist::default();
        let test_addr = "0x1234567890123456789012345678901234567890";
        whitelist.blacklist.pools.push(BlacklistPool {
            address: test_addr.to_string(),
            pair: "TEST/USDC".to_string(),
            dex: "UniswapV3".to_string(),
            fee_tier: 500,
            reason: "Test".to_string(),
            phantom_spread: None,
            date_added: "2026-01-28".to_string(),
            discovered_by: None,
        });
        
        let addr = Address::from_str(test_addr).unwrap();
        assert!(whitelist.is_pool_blacklisted(&addr));
    }
    
    #[test]
    fn test_whitelist_enforcement_strict() {
        let mut whitelist = PoolWhitelist::default();
        whitelist.config.whitelist_enforcement = "strict".to_string();
        
        let test_addr = Address::from_str(
            "0x0000000000000000000000000000000000000001"
        ).unwrap();
        
        // Not in whitelist, strict mode = rejected
        assert!(!whitelist.is_pool_allowed(&test_addr, 500, "TEST/USDC"));
    }
    
    #[test]
    fn test_scoring_new_pool() {
        let score = PoolScore::new(
            "0x1234",
            "WMATIC/USDC",
            "UniswapV3",
            500,
            0.5,
        );
        assert_eq!(score.score, 0.5);
        assert_eq!(score.metrics.total_attempts, 0);
    }
    
    #[test]
    fn test_scoring_update_success() {
        let mut scores = PoolScores::default();
        
        scores.update_after_execution(
            "0x1234",
            "TEST/USDC",
            "UniswapV3",
            500,
            true,  // success
            Some(1000),
            Some(980),
        );
        
        let score = scores.get_score("0x1234");
        assert!(score > 0.5); // Should improve from default
    }
    
    #[test]
    fn test_scoring_update_failure() {
        let mut scores = PoolScores::default();
        
        scores.update_after_execution(
            "0x1234",
            "TEST/USDC",
            "UniswapV3",
            500,
            false,  // failure
            Some(1000),
            Some(500),
        );
        
        let score = scores.get_score("0x1234");
        assert!(score < 0.5); // Should decrease from default
    }
}
```

#### **Integration Tests**

```rust
// tests/integration_tests.rs

#[cfg(test)]
mod integration_tests {
    use super::*;
    
    #[tokio::test]
    async fn test_full_filter_pipeline() {
        // Test: opportunity goes through all filters
        
        // 1. Load whitelist
        let whitelist = PoolWhitelist::load().unwrap();
        
        // 2. Load pool scores
        let pool_scores = PoolScores::load().unwrap();
        
        // 3. Create test pool
        let test_pool = TestPool {
            address: "0x86f1d8390222a3691c28938ec7404a1661e618e0".parse().unwrap(),
            fee_tier: 500,
            liquidity: 10_000_000_000,
            pair: "WMATIC/USDC".to_string(),
        };
        
        // 4. Apply filters
        assert!(whitelist.is_pool_allowed(&test_pool.address, test_pool.fee_tier, &test_pool.pair));
        
        let min_liq = get_min_liquidity_threshold(test_pool.fee_tier);
        assert!(test_pool.liquidity >= min_liq);
        
        let score = pool_scores.get_score(&format!("{:?}", test_pool.address));
        assert!(score >= 0.4);
        
        // All filters passed!
    }
    
    #[tokio::test]
    async fn test_score_persistence() {
        // Test: scores persist across restarts
        
        let mut scores = PoolScores::default();
        scores.update_after_execution(
            "0xtest123",
            "TEST/USDC",
            "UniswapV3",
            500,
            true,
            Some(1000),
            Some(990),
        );
        
        scores.save().unwrap();
        
        // Reload
        let reloaded = PoolScores::load().unwrap();
        assert_eq!(
            reloaded.get_score("0xtest123"),
            scores.get_score("0xtest123")
        );
    }
}
```

#### **Paper Trading Validation**

```bash
# Run paper trading for 24 hours with Phase 1 filters

# 1. Enable all filters in config
# 2. Run paper trading
cargo run --release -- --mode paper

# 3. Monitor metrics
tail -f logs/paper_trading.log | grep "Filtered\|Score\|Whitelist"

# 4. After 24 hours, analyze results
python scripts/analyze_phase1_impact.py

# Expected output:
# - Opportunity count reduction
# - False positive rate
# - RPC call count
# - Scoring convergence rate
```

### **Phase 2 Testing**

#### **Unit Tests**

```rust
// tests/multicall_tests.rs

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_call_encoding() {
        let verifier = create_test_verifier();
        
        let call = verifier.build_quote_call(
            test_token_a(),
            test_token_b(),
            U256::from(1000),
            500,
        ).unwrap();
        
        assert_eq!(call.target, verifier.quoter_address);
        assert!(call.allow_failure);
        assert!(call.call_data.len() > 0);
    }
    
    #[test]
    fn test_result_decoding() {
        let verifier = create_test_verifier();
        
        // Mock Quoter response: uint256(12345)
        let mut data = vec![0u8; 32];
        U256::from(12345).to_big_endian(&mut data);
        
        let amount = verifier.decode_quote_result(&Bytes::from(data)).unwrap();
        assert_eq!(amount, U256::from(12345));
    }
    
    #[tokio::test]
    async fn test_batch_size_limits() {
        let manager = create_test_batch_manager();
        
        // Add 25 opportunities (exceeds max of 20)
        for i in 0..25 {
            manager.add_opportunity(create_test_opportunity(i)).await;
        }
        
        // Should have flushed once
        // Verify only 5 remain in pending
        assert_eq!(manager.pending_count().await, 5);
    }
}
```

#### **Integration Tests**

```rust
// tests/multicall_integration_tests.rs

#[tokio::test]
async fn test_multicall_vs_sequential_accuracy() {
    // Test: Multicall produces same results as sequential
    
    let provider = create_test_provider().await;
    let multicall_verifier = MulticallVerifier::new(provider.clone()).unwrap();
    
    let opportunities = vec![
        create_test_opportunity(1),
        create_test_opportunity(2),
        create_test_opportunity(3),
    ];
    
    // Multicall verification
    let multicall_results = multicall_verifier
        .batch_quote_opportunities(opportunities.clone())
        .await
        .unwrap();
    
    // Sequential verification (for comparison)
    let sequential_results = verify_sequentially(provider, opportunities).await;
    
    // Compare results
    assert_eq!(multicall_results.len(), sequential_results.len());
    
    for (mc, seq) in multicall_results.iter().zip(sequential_results.iter()) {
        assert_eq!(mc.quoted_mid, seq.quoted_mid);
        assert_eq!(mc.quoted_out, seq.quoted_out);
        assert_eq!(mc.verified, seq.verified);
    }
}

#[tokio::test]
async fn test_batch_manager_timing() {
    // Test: Batch flushes after timeout
    
    let config = BatchConfig {
        max_batch_size: 20,
        max_wait_ms: 1000,
        min_batch_size: 1,
    };
    
    let manager = create_test_batch_manager_with_config(config);
    manager.clone().start_background_flusher().await;
    
    // Add 1 opportunity (not enough to trigger size flush)
    manager.add_opportunity(create_test_opportunity(1)).await;
    
    // Wait for timeout + buffer
    tokio::time::sleep(Duration::from_millis(1200)).await;
    
    // Batch should have been flushed
    assert_eq!(manager.pending_count().await, 0);
}
```

#### **Performance Testing**

```bash
# Measure RPC call reduction

# 1. Run with sequential Quoter (baseline)
cargo run --release -- --mode paper --sequential-quoter

# Count RPC calls over 1 hour
# Expected: ~328 calls

# 2. Run with Multicall batching
cargo run --release -- --mode paper --multicall

# Count RPC calls over 1 hour
# Expected: ~10-15 calls

# 3. Measure latency
python scripts/measure_latency.py

# Expected:
# Sequential: ~400ms per opportunity
# Multicall: ~5-10ms per opportunity
```

---

## ðŸ“Š SUCCESS CRITERIA

### **Phase 1 Success Criteria**

```
MUST ACHIEVE (Required):
âœ… False positive rate reduced to <25% (from 80%)
âœ… RPC calls reduced by >50% (from 860/hour)
âœ… Zero real opportunities filtered (no false negatives)
âœ… Pool scoring system learns from executions
âœ… All unit tests passing
âœ… 24-hour paper trading validation successful

STRETCH GOALS (Nice to have):
â­ False positive rate <18%
â­ RPC calls reduced by >60%
â­ Automated whitelist updates (from subgraph)
â­ Score convergence within 10 executions
â­ Integration tests >90% coverage

METRICS TO TRACK:
â”œâ”€ Opportunities detected (before/after)
â”œâ”€ False positive rate (before/after)
â”œâ”€ RPC call count (before/after)
â”œâ”€ Scoring accuracy (predicted vs actual)
â”œâ”€ Filter effectiveness (per filter)
â””â”€ System stability (uptime, errors)
```

### **Phase 2 Success Criteria**

```
MUST ACHIEVE (Required):
âœ… RPC calls reduced by >90% from Phase 1 baseline
âœ… Latency <100ms per batch (vs 400ms sequential)
âœ… Multicall accuracy = sequential accuracy (Â±1%)
âœ… Graceful fallback on Multicall errors
âœ… All unit tests passing
âœ… 48-hour paper trading validation successful

STRETCH GOALS (Nice to have):
â­ RPC calls reduced by >95%
â­ Latency <50ms per batch
â­ Zero failed batches in 48-hour test
â­ Adaptive batching optimizes automatically
â­ Can handle >50 opportunities/batch

METRICS TO TRACK:
â”œâ”€ RPC call count (before/after Multicall)
â”œâ”€ Latency per opportunity (before/after)
â”œâ”€ Batch success rate
â”œâ”€ Batch size distribution
â”œâ”€ Adaptive batching effectiveness
â””â”€ Accuracy comparison (Multicall vs sequential)
```

### **Combined Phase 1 & 2 Success**

```
FINAL SYSTEM REQUIREMENTS:
âœ… False positive rate <20%
âœ… RPC calls <20/hour (from 860/hour)
âœ… Latency <10ms per opportunity check
âœ… System learns and improves over time
âœ… 7-day continuous operation without issues
âœ… Profitable in live testing ($100 capital)

OVERALL IMPACT:
â”œâ”€ 95%+ reduction in RPC calls
â”œâ”€ 75%+ reduction in false positives
â”œâ”€ 95%+ reduction in verification latency
â”œâ”€ Self-improving pool quality system
â””â”€ Ready for V4 multi-pair expansion
```

---

## ðŸ“‹ DEPLOYMENT CHECKLIST

### **Pre-Deployment**

```
CODE PREPARATION:
[ ] All Phase 1 code implemented and tested
[ ] All Phase 2 code implemented and tested
[ ] Unit tests passing (>90% coverage)
[ ] Integration tests passing
[ ] Paper trading validated (7 days)

CONFIGURATION:
[ ] pools_whitelist.json created and validated
[ ] Initial pool scores seeded (if any)
[ ] Batch configuration tuned
[ ] Monitoring configured

INFRASTRUCTURE:
[ ] Alchemy upgraded (if using paid tier)
[ ] Backup RPC endpoints configured
[ ] Logging enhanced
[ ] Alerting configured

DOCUMENTATION:
[ ] Implementation documented
[ ] Configuration guide written
[ ] Troubleshooting guide created
[ ] Maintenance procedures documented

SAFETY:
[ ] Test wallet prepared ($100-200)
[ ] Stop-loss configured
[ ] Emergency stop procedure tested
[ ] Rollback plan documented
```

### **Deployment Steps**

```
STEP 1: DEPLOY PHASE 1 (Day 11)
â”œâ”€ 1.1: Deploy whitelist filtering
â”œâ”€ 1.2: Deploy enhanced liquidity thresholds
â”œâ”€ 1.3: Deploy pool scoring
â”œâ”€ 1.4: Monitor for 24 hours
â””â”€ 1.5: Validate metrics meet success criteria

STEP 2: DEPLOY PHASE 2 (Day 12)
â”œâ”€ 2.1: Deploy Multicall verification
â”œâ”€ 2.2: Deploy batch manager
â”œâ”€ 2.3: Monitor for 24 hours
â”œâ”€ 2.4: Validate RPC reduction
â””â”€ 2.5: Validate accuracy maintained

STEP 3: INTEGRATED TESTING (Day 13)
â”œâ”€ 3.1: Full system paper trading
â”œâ”€ 3.2: Measure all metrics
â”œâ”€ 3.3: Tune parameters
â””â”€ 3.4: Final validation

STEP 4: LIVE TESTING (Day 14)
â”œâ”€ 4.1: Deploy with $100 test capital
â”œâ”€ 4.2: Monitor first 10 trades closely
â”œâ”€ 4.3: Validate profitability
â””â”€ 4.4: Scale if successful

STEP 5: PRODUCTION (Day 15+)
â”œâ”€ 5.1: Scale to $500 if Day 14 successful
â”œâ”€ 5.2: Continue monitoring
â”œâ”€ 5.3: Iterate and improve
â””â”€ 5.4: Plan V4 (multi-pair expansion)
```

---

## ðŸ” MONITORING & MAINTENANCE

### **Daily Monitoring**

```bash
# Daily health check script
#!/bin/bash

echo "=== Phase 1 & 2 Daily Health Check ==="
echo ""

# 1. Check whitelist status
echo "Whitelist Status:"
jq '.whitelist.pools | length' config/pools_whitelist.json
jq '.blacklist.pools | length' config/pools_whitelist.json

# 2. Check pool scores
echo ""
echo "Pool Scores:"
jq '.scores | length' data/pool_scores.json
jq '[.scores[] | .score] | add / length' data/pool_scores.json  # Average score

# 3. Check RPC call count
echo ""
echo "RPC Calls (last 24h):"
grep "RPC call" logs/*.log | wc -l

# 4. Check opportunities
echo ""
echo "Opportunities (last 24h):"
psql -d dexarb_db -c "
SELECT 
    COUNT(*) as total_opportunities,
    COUNT(CASE WHEN executed THEN 1 END) as executed,
    COUNT(CASE WHEN success THEN 1 END) as successful
FROM opportunities
WHERE timestamp > NOW() - INTERVAL '24 hours';"

# 5. Check scores trending
echo ""
echo "Score Trends:"
python scripts/analyze_score_trends.py
```

### **Weekly Maintenance**

```
WEEKLY TASKS:
[ ] Review pool scores (remove underperformers)
[ ] Update whitelist (add new high-volume pools)
[ ] Analyze false positives (any patterns?)
[ ] Tune batch parameters (if needed)
[ ] Review RPC usage (within limits?)
[ ] Check for new opportunities (V4 planning)
[ ] Update documentation
[ ] Backup configuration files
```

### **Monthly Review**

```
MONTHLY REVIEW:
[ ] Full performance analysis
[ ] Compare against targets
[ ] Identify optimization opportunities
[ ] Review and update whitelist
[ ] Score system effectiveness analysis
[ ] Multicall performance review
[ ] Plan next iteration (V4, etc.)
[ ] Update success criteria if needed
```

---

## ðŸŽ¯ NEXT STEPS AFTER PHASE 1 & 2

### **V4 Planning**

Once Phase 1 & 2 are stable:

```
V4 EXPANSION:
1. Add WMATIC/USDC pair (highest priority)
2. Add WETH/USDC pair (second priority)
3. Expand whitelist to 10-15 pools
4. Increase opportunity flow 3-5x
5. Expected: $500-1,000/day (vs $200-400 currently)

INFRASTRUCTURE:
â”œâ”€ Multi-RPC implementation (if needed)
â”œâ”€ WebSocket subscriptions (if needed)
â”œâ”€ Advanced slippage modeling
â””â”€ Dynamic threshold adjustment

OPTIMIZATION:
â”œâ”€ Machine learning for scoring
â”œâ”€ Predictive opportunity detection
â”œâ”€ Gas optimization strategies
â””â”€ Automated pair discovery
```

---

## ðŸ“š APPENDIX

### **A. Configuration File Examples**

See code sections above for complete examples.

### **B. Useful Commands**

```bash
# Load whitelist
jq '.' config/pools_whitelist.json

# Add pool to whitelist manually
jq '.whitelist.pools += [{"address": "0x...", ...}]' config/pools_whitelist.json > tmp.json && mv tmp.json config/pools_whitelist.json

# Check pool score
jq '.scores["0x..."]' data/pool_scores.json

# Count RPC calls
grep "Multicall" logs/*.log | wc -l

# Monitor batch sizes
grep "Flushing batch" logs/*.log | awk '{print $5}' | sort | uniq -c
```

### **C. Troubleshooting Guide**

```
COMMON ISSUES:

1. Whitelist Not Loading
   â”œâ”€ Check file exists: ls config/pools_whitelist.json
   â”œâ”€ Validate JSON: jq '.' config/pools_whitelist.json
   â””â”€ Check permissions: ls -l config/

2. Pool Scores Not Persisting
   â”œâ”€ Check directory: ls data/
   â”œâ”€ Check permissions: ls -l data/
   â””â”€ Check disk space: df -h

3. Multicall Failing
   â”œâ”€ Check Alchemy connection
   â”œâ”€ Verify Multicall3 address
   â”œâ”€ Check batch size (not too large)
   â””â”€ Review error logs

4. Scores Not Updating
   â”œâ”€ Verify executor integration
   â”œâ”€ Check slippage tracking
   â””â”€ Review score calculation logic

5. Too Many Opportunities Filtered
   â”œâ”€ Review whitelist (too strict?)
   â”œâ”€ Check liquidity thresholds
   â”œâ”€ Review pool scores (decaying?)
   â””â”€ Adjust thresholds if needed
```

---

## ðŸŽ‰ CONCLUSION

This optimization plan provides:

1. âœ… **Phase 1**: Intelligent filtering (whitelist, liquidity, scoring)
   - 62% reduction in noise
   - 77% improvement in false positive rate
   - Self-improving system

2. âœ… **Phase 2**: Multicall batching
   - 95% reduction in RPC calls
   - 95% reduction in latency
   - Production-ready implementation

3. âœ… **Complete Code**: Ready to implement
   - All files provided
   - Tested patterns
   - Industry-standard approach

4. âœ… **Clear Timeline**: 2 weeks to completion
   - Week 1: Phase 1
   - Week 2: Phase 2
   - Validated and tested

5. âœ… **Path to V4**: Multi-pair expansion
   - Foundation for growth
   - Scalable architecture
   - 3-5x opportunity increase

**Ready to start implementation!** ðŸš€

**Questions or need clarification on any component?**

---

**Document Version**: 1.0  
**Last Updated**: 2026-01-28  
**Next Review**: After Phase 1 completion
